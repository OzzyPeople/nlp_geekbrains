{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка текста с помощью Python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Введение в библиотеку re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Регулярное выражение — это последовательность символов, используемая для поиска и замены текста в строке или файле.\n",
    "\n",
    "Регулярные выражения используют два типа символов:\n",
    "\n",
    "- специальные символы: как следует из названия, у этих символов есть специальные значения. \n",
    "- литералы (например: a, b, 1, 2 и т. д.).\n",
    "\n",
    "Примеры регулярных выражений:\n",
    "\n",
    "- . – любой символ, кроме перевода строки;\n",
    "- \\w – одно слово;\n",
    "- \\d – одна цифра;\n",
    "- \\s – один пробел;\n",
    "- \\W – одно НЕслово;\n",
    "- \\D – одна НЕцифра;\n",
    "- \\S – один НЕпробел;\n",
    "- [abc] – находит любой из указанных символов match any of a, b, or c;\n",
    "- [^abc] – находит любой символ, кроме указанных;\n",
    "- [a-g] – находит символ в промежутке от a до g.\n",
    "\n",
    "\n",
    "В Python для работы с регулярными выражениями есть модуль re. Для использования его нужно импортировать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module re:\n",
      "\n",
      "NAME\n",
      "    re - Support for regular expressions (RE).\n",
      "\n",
      "MODULE REFERENCE\n",
      "    https://docs.python.org/3.8/library/re\n",
      "    \n",
      "    The following documentation is automatically generated from the Python\n",
      "    source files.  It may be incomplete, incorrect or include features that\n",
      "    are considered implementation detail and may vary between Python\n",
      "    implementations.  When in doubt, consult the module reference at the\n",
      "    location listed above.\n",
      "\n",
      "DESCRIPTION\n",
      "    This module provides regular expression matching operations similar to\n",
      "    those found in Perl.  It supports both 8-bit and Unicode strings; both\n",
      "    the pattern and the strings being processed can contain null bytes and\n",
      "    characters outside the US ASCII range.\n",
      "    \n",
      "    Regular expressions can contain both special and ordinary characters.\n",
      "    Most ordinary characters, like \"A\", \"a\", or \"0\", are the simplest\n",
      "    regular expressions; they simply match themselves.  You can\n",
      "    concatenate ordinary characters, so last matches the string 'last'.\n",
      "    \n",
      "    The special characters are:\n",
      "        \".\"      Matches any character except a newline.\n",
      "        \"^\"      Matches the start of the string.\n",
      "        \"$\"      Matches the end of the string or just before the newline at\n",
      "                 the end of the string.\n",
      "        \"*\"      Matches 0 or more (greedy) repetitions of the preceding RE.\n",
      "                 Greedy means that it will match as many repetitions as possible.\n",
      "        \"+\"      Matches 1 or more (greedy) repetitions of the preceding RE.\n",
      "        \"?\"      Matches 0 or 1 (greedy) of the preceding RE.\n",
      "        *?,+?,?? Non-greedy versions of the previous three special characters.\n",
      "        {m,n}    Matches from m to n repetitions of the preceding RE.\n",
      "        {m,n}?   Non-greedy version of the above.\n",
      "        \"\\\\\"     Either escapes special characters or signals a special sequence.\n",
      "        []       Indicates a set of characters.\n",
      "                 A \"^\" as the first character indicates a complementing set.\n",
      "        \"|\"      A|B, creates an RE that will match either A or B.\n",
      "        (...)    Matches the RE inside the parentheses.\n",
      "                 The contents can be retrieved or matched later in the string.\n",
      "        (?aiLmsux) The letters set the corresponding flags defined below.\n",
      "        (?:...)  Non-grouping version of regular parentheses.\n",
      "        (?P<name>...) The substring matched by the group is accessible by name.\n",
      "        (?P=name)     Matches the text matched earlier by the group named name.\n",
      "        (?#...)  A comment; ignored.\n",
      "        (?=...)  Matches if ... matches next, but doesn't consume the string.\n",
      "        (?!...)  Matches if ... doesn't match next.\n",
      "        (?<=...) Matches if preceded by ... (must be fixed length).\n",
      "        (?<!...) Matches if not preceded by ... (must be fixed length).\n",
      "        (?(id/name)yes|no) Matches yes pattern if the group with id/name matched,\n",
      "                           the (optional) no pattern otherwise.\n",
      "    \n",
      "    The special sequences consist of \"\\\\\" and a character from the list\n",
      "    below.  If the ordinary character is not on the list, then the\n",
      "    resulting RE will match the second character.\n",
      "        \\number  Matches the contents of the group of the same number.\n",
      "        \\A       Matches only at the start of the string.\n",
      "        \\Z       Matches only at the end of the string.\n",
      "        \\b       Matches the empty string, but only at the start or end of a word.\n",
      "        \\B       Matches the empty string, but not at the start or end of a word.\n",
      "        \\d       Matches any decimal digit; equivalent to the set [0-9] in\n",
      "                 bytes patterns or string patterns with the ASCII flag.\n",
      "                 In string patterns without the ASCII flag, it will match the whole\n",
      "                 range of Unicode digits.\n",
      "        \\D       Matches any non-digit character; equivalent to [^\\d].\n",
      "        \\s       Matches any whitespace character; equivalent to [ \\t\\n\\r\\f\\v] in\n",
      "                 bytes patterns or string patterns with the ASCII flag.\n",
      "                 In string patterns without the ASCII flag, it will match the whole\n",
      "                 range of Unicode whitespace characters.\n",
      "        \\S       Matches any non-whitespace character; equivalent to [^\\s].\n",
      "        \\w       Matches any alphanumeric character; equivalent to [a-zA-Z0-9_]\n",
      "                 in bytes patterns or string patterns with the ASCII flag.\n",
      "                 In string patterns without the ASCII flag, it will match the\n",
      "                 range of Unicode alphanumeric characters (letters plus digits\n",
      "                 plus underscore).\n",
      "                 With LOCALE, it will match the set [0-9_] plus characters defined\n",
      "                 as letters for the current locale.\n",
      "        \\W       Matches the complement of \\w.\n",
      "        \\\\       Matches a literal backslash.\n",
      "    \n",
      "    This module exports the following functions:\n",
      "        match     Match a regular expression pattern to the beginning of a string.\n",
      "        fullmatch Match a regular expression pattern to all of a string.\n",
      "        search    Search a string for the presence of a pattern.\n",
      "        sub       Substitute occurrences of a pattern found in a string.\n",
      "        subn      Same as sub, but also return the number of substitutions made.\n",
      "        split     Split a string by the occurrences of a pattern.\n",
      "        findall   Find all occurrences of a pattern in a string.\n",
      "        finditer  Return an iterator yielding a Match object for each match.\n",
      "        compile   Compile a pattern into a Pattern object.\n",
      "        purge     Clear the regular expression cache.\n",
      "        escape    Backslash all non-alphanumerics in a string.\n",
      "    \n",
      "    Each function other than purge and escape can take an optional 'flags' argument\n",
      "    consisting of one or more of the following module constants, joined by \"|\".\n",
      "    A, L, and U are mutually exclusive.\n",
      "        A  ASCII       For string patterns, make \\w, \\W, \\b, \\B, \\d, \\D\n",
      "                       match the corresponding ASCII character categories\n",
      "                       (rather than the whole Unicode categories, which is the\n",
      "                       default).\n",
      "                       For bytes patterns, this flag is the only available\n",
      "                       behaviour and needn't be specified.\n",
      "        I  IGNORECASE  Perform case-insensitive matching.\n",
      "        L  LOCALE      Make \\w, \\W, \\b, \\B, dependent on the current locale.\n",
      "        M  MULTILINE   \"^\" matches the beginning of lines (after a newline)\n",
      "                       as well as the string.\n",
      "                       \"$\" matches the end of lines (before a newline) as well\n",
      "                       as the end of the string.\n",
      "        S  DOTALL      \".\" matches any character at all, including the newline.\n",
      "        X  VERBOSE     Ignore whitespace and comments for nicer looking RE's.\n",
      "        U  UNICODE     For compatibility only. Ignored for string patterns (it\n",
      "                       is the default), and forbidden for bytes patterns.\n",
      "    \n",
      "    This module also defines an exception 'error'.\n",
      "\n",
      "CLASSES\n",
      "    builtins.Exception(builtins.BaseException)\n",
      "        error\n",
      "    builtins.object\n",
      "        Match\n",
      "        Pattern\n",
      "    \n",
      "    class Match(builtins.object)\n",
      "     |  The result of re.match() and re.search().\n",
      "     |  Match objects always have a boolean value of True.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __copy__(self, /)\n",
      "     |  \n",
      "     |  __deepcopy__(self, memo, /)\n",
      "     |  \n",
      "     |  __getitem__(self, key, /)\n",
      "     |      Return self[key].\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  end(self, group=0, /)\n",
      "     |      Return index of the end of the substring matched by group.\n",
      "     |  \n",
      "     |  expand(self, /, template)\n",
      "     |      Return the string obtained by doing backslash substitution on the string template, as done by the sub() method.\n",
      "     |  \n",
      "     |  group(...)\n",
      "     |      group([group1, ...]) -> str or tuple.\n",
      "     |      Return subgroup(s) of the match by indices or names.\n",
      "     |      For 0 returns the entire match.\n",
      "     |  \n",
      "     |  groupdict(self, /, default=None)\n",
      "     |      Return a dictionary containing all the named subgroups of the match, keyed by the subgroup name.\n",
      "     |      \n",
      "     |      default\n",
      "     |        Is used for groups that did not participate in the match.\n",
      "     |  \n",
      "     |  groups(self, /, default=None)\n",
      "     |      Return a tuple containing all the subgroups of the match, from 1.\n",
      "     |      \n",
      "     |      default\n",
      "     |        Is used for groups that did not participate in the match.\n",
      "     |  \n",
      "     |  span(self, group=0, /)\n",
      "     |      For match object m, return the 2-tuple (m.start(group), m.end(group)).\n",
      "     |  \n",
      "     |  start(self, group=0, /)\n",
      "     |      Return index of the start of the substring matched by group.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  endpos\n",
      "     |      The index into the string beyond which the RE engine will not go.\n",
      "     |  \n",
      "     |  lastgroup\n",
      "     |      The name of the last matched capturing group.\n",
      "     |  \n",
      "     |  lastindex\n",
      "     |      The integer index of the last matched capturing group.\n",
      "     |  \n",
      "     |  pos\n",
      "     |      The index into the string at which the RE engine started looking for a match.\n",
      "     |  \n",
      "     |  re\n",
      "     |      The regular expression object.\n",
      "     |  \n",
      "     |  regs\n",
      "     |  \n",
      "     |  string\n",
      "     |      The string passed to match() or search().\n",
      "    \n",
      "    class Pattern(builtins.object)\n",
      "     |  Compiled regular expression object.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __copy__(self, /)\n",
      "     |  \n",
      "     |  __deepcopy__(self, memo, /)\n",
      "     |  \n",
      "     |  __eq__(self, value, /)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __hash__(self, /)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  findall(self, /, string, pos=0, endpos=9223372036854775807)\n",
      "     |      Return a list of all non-overlapping matches of pattern in string.\n",
      "     |  \n",
      "     |  finditer(self, /, string, pos=0, endpos=9223372036854775807)\n",
      "     |      Return an iterator over all non-overlapping matches for the RE pattern in string.\n",
      "     |      \n",
      "     |      For each match, the iterator returns a match object.\n",
      "     |  \n",
      "     |  fullmatch(self, /, string, pos=0, endpos=9223372036854775807)\n",
      "     |      Matches against all of the string.\n",
      "     |  \n",
      "     |  match(self, /, string, pos=0, endpos=9223372036854775807)\n",
      "     |      Matches zero or more characters at the beginning of the string.\n",
      "     |  \n",
      "     |  scanner(self, /, string, pos=0, endpos=9223372036854775807)\n",
      "     |  \n",
      "     |  search(self, /, string, pos=0, endpos=9223372036854775807)\n",
      "     |      Scan through string looking for a match, and return a corresponding match object instance.\n",
      "     |      \n",
      "     |      Return None if no position in the string matches.\n",
      "     |  \n",
      "     |  split(self, /, string, maxsplit=0)\n",
      "     |      Split string by the occurrences of pattern.\n",
      "     |  \n",
      "     |  sub(self, /, repl, string, count=0)\n",
      "     |      Return the string obtained by replacing the leftmost non-overlapping occurrences of pattern in string by the replacement repl.\n",
      "     |  \n",
      "     |  subn(self, /, repl, string, count=0)\n",
      "     |      Return the tuple (new_string, number_of_subs_made) found by replacing the leftmost non-overlapping occurrences of pattern with the replacement repl.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  flags\n",
      "     |      The regex matching flags.\n",
      "     |  \n",
      "     |  groupindex\n",
      "     |      A dictionary mapping group names to group numbers.\n",
      "     |  \n",
      "     |  groups\n",
      "     |      The number of capturing groups in the pattern.\n",
      "     |  \n",
      "     |  pattern\n",
      "     |      The pattern string from which the RE object was compiled.\n",
      "    \n",
      "    class error(builtins.Exception)\n",
      "     |  error(msg, pattern=None, pos=None)\n",
      "     |  \n",
      "     |  Exception raised for invalid regular expressions.\n",
      "     |  \n",
      "     |  Attributes:\n",
      "     |  \n",
      "     |      msg: The unformatted error message\n",
      "     |      pattern: The regular expression pattern\n",
      "     |      pos: The index in the pattern where compilation failed (may be None)\n",
      "     |      lineno: The line corresponding to pos (may be None)\n",
      "     |      colno: The column corresponding to pos (may be None)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      error\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, msg, pattern=None, pos=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.Exception:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "\n",
      "FUNCTIONS\n",
      "    compile(pattern, flags=0)\n",
      "        Compile a regular expression pattern, returning a Pattern object.\n",
      "    \n",
      "    escape(pattern)\n",
      "        Escape special characters in a string.\n",
      "    \n",
      "    findall(pattern, string, flags=0)\n",
      "        Return a list of all non-overlapping matches in the string.\n",
      "        \n",
      "        If one or more capturing groups are present in the pattern, return\n",
      "        a list of groups; this will be a list of tuples if the pattern\n",
      "        has more than one group.\n",
      "        \n",
      "        Empty matches are included in the result.\n",
      "    \n",
      "    finditer(pattern, string, flags=0)\n",
      "        Return an iterator over all non-overlapping matches in the\n",
      "        string.  For each match, the iterator returns a Match object.\n",
      "        \n",
      "        Empty matches are included in the result.\n",
      "    \n",
      "    fullmatch(pattern, string, flags=0)\n",
      "        Try to apply the pattern to all of the string, returning\n",
      "        a Match object, or None if no match was found.\n",
      "    \n",
      "    match(pattern, string, flags=0)\n",
      "        Try to apply the pattern at the start of the string, returning\n",
      "        a Match object, or None if no match was found.\n",
      "    \n",
      "    purge()\n",
      "        Clear the regular expression caches\n",
      "    \n",
      "    search(pattern, string, flags=0)\n",
      "        Scan through string looking for a match to the pattern, returning\n",
      "        a Match object, or None if no match was found.\n",
      "    \n",
      "    split(pattern, string, maxsplit=0, flags=0)\n",
      "        Split the source string by the occurrences of the pattern,\n",
      "        returning a list containing the resulting substrings.  If\n",
      "        capturing parentheses are used in pattern, then the text of all\n",
      "        groups in the pattern are also returned as part of the resulting\n",
      "        list.  If maxsplit is nonzero, at most maxsplit splits occur,\n",
      "        and the remainder of the string is returned as the final element\n",
      "        of the list.\n",
      "    \n",
      "    sub(pattern, repl, string, count=0, flags=0)\n",
      "        Return the string obtained by replacing the leftmost\n",
      "        non-overlapping occurrences of the pattern in string by the\n",
      "        replacement repl.  repl can be either a string or a callable;\n",
      "        if a string, backslash escapes in it are processed.  If it is\n",
      "        a callable, it's passed the Match object and must return\n",
      "        a replacement string to be used.\n",
      "    \n",
      "    subn(pattern, repl, string, count=0, flags=0)\n",
      "        Return a 2-tuple containing (new_string, number).\n",
      "        new_string is the string obtained by replacing the leftmost\n",
      "        non-overlapping occurrences of the pattern in the source\n",
      "        string by the replacement repl.  number is the number of\n",
      "        substitutions that were made. repl can be either a string or a\n",
      "        callable; if a string, backslash escapes in it are processed.\n",
      "        If it is a callable, it's passed the Match object and must\n",
      "        return a replacement string to be used.\n",
      "    \n",
      "    template(pattern, flags=0)\n",
      "        Compile a template pattern, returning a Pattern object\n",
      "\n",
      "DATA\n",
      "    A = re.ASCII\n",
      "    ASCII = re.ASCII\n",
      "    DOTALL = re.DOTALL\n",
      "    I = re.IGNORECASE\n",
      "    IGNORECASE = re.IGNORECASE\n",
      "    L = re.LOCALE\n",
      "    LOCALE = re.LOCALE\n",
      "    M = re.MULTILINE\n",
      "    MULTILINE = re.MULTILINE\n",
      "    S = re.DOTALL\n",
      "    U = re.UNICODE\n",
      "    UNICODE = re.UNICODE\n",
      "    VERBOSE = re.VERBOSE\n",
      "    X = re.VERBOSE\n",
      "    __all__ = ['match', 'fullmatch', 'search', 'sub', 'subn', 'split', 'fi...\n",
      "\n",
      "VERSION\n",
      "    2.2.1\n",
      "\n",
      "FILE\n",
      "    /home/rzaharov@mvs.local/.pyenv/versions/3.8.3/lib/python3.8/re.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "help(re)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чаще всего регулярные выражения используются для:\n",
    "- поиска в строке;\n",
    "- разбиения строки на подстроки;\n",
    "- замены части строки.\n",
    "\n",
    "Давайте посмотрим на методы, которые предоставляет библиотека re для этих задач."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### re.match(pattern, string):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот метод ищет по заданному шаблону в начале строки. Например, если мы вызовем метод match() на строке «find matched pattern» с шаблоном «find», то он завершится успешно. Однако если мы будем искать «matched», то результат будет отрицательный."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 4), match='find'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match(r'find', 'find matched pattern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(re.match(r'matched', 'find matched pattern'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='f'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match(r'[a-z]', 'find matched pattern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 4), match='find'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match(r'[a-z]+', 'find matched pattern')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### re.search(pattern, string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот метод похож на match(), но он ищет не только в начале строки. В отличие от предыдущего, search() вернет объект, если мы попытаемся найти «matched»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched\n"
     ]
    }
   ],
   "source": [
    "result = re.search(r'matched', 'find matched pattern')\n",
    "print(result.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 4), match='find'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'\\w+', 'find matched pattern')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### re.findall(pattern, string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот метод возвращает список всех найденных совпадений. У метода findall() нет ограничений на поиск в начале или конце строки. Для поиска рекомендуется использовать именно findall(), так как он может работать и как re.search(), и как re.match()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['re', 're', 're']\n"
     ]
    }
   ],
   "source": [
    "result = re.findall(r're', 'we want re to find all re in this text re')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вернуть список доменов из списка адресов электронной почты.\n",
    "Online regex tester and debugger: https://regex101.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@gmail.com', '@test.in', '@mail.ru', '@hjk*fhjd']\n"
     ]
    }
   ],
   "source": [
    "result = re.findall(r'@\\w+.\\w+', 'dsdf, abc.test@gmail.com, xyz@test.in, test.first@mail.ru, dhjsd@hjk*fhjd')\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Извлечь дату из строки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12-05-2007', '11-11-2011', '12-01-2009']\n"
     ]
    }
   ],
   "source": [
    "result = re.findall(r'\\d{2}-\\d{2}-\\d{4}', 'Name 34-3456 12-05-2007, XYZ 56-4532 11-11-2011, ABC 67-8945 12-01-2009')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### re.split(pattern, string, [maxsplit=0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот метод разделяет строку по заданному шаблону."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'want', 'to', 'devide']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r' ', 'we want to devide')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### re.sub(pattern, repl, string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот метод ищет шаблон в строке и заменяет его на указанную подстроку. Если шаблон не найден, строка остается неизменной.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to be the best nlp specialist in the World\n"
     ]
    }
   ],
   "source": [
    "result = re.sub(r'Russia', 'the World', 'I want to be the best nlp specialist in Russia')\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Удаление пунктуации\n",
    "\n",
    "Мы можем использовать регулярные выражения для дополнительного фильтрова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Russian is an East Slavic language which is an official language in Russia Belarus Kazakhstan Kyrgyzstan as well as being widely used throughout Eastern Europe the Baltic states the Caucasus and Central Asia'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Russian is an East Slavic language, which is an official language in Russia, Belarus, Kazakhstan, Kyrgyzstan, as well as being widely used throughout Eastern Europe, the Baltic states, the Caucasus and Central Asia.'\n",
    "no_punctuation_text = re.sub(r'[^\\w\\s]', '', text)\n",
    "no_punctuation_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\t\\n\\r\\x0b\\x0c'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Russian is an East Slavic language which is an official language in Russia Belarus Kazakhstan Kyrgyzstan as well as being widely used throughout Eastern Europe the Baltic states the Caucasus and Central Asia'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join([ch for ch in list(text) if ch not in string.punctuation])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Токенизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Привести к нижнему регистру  .lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'This is, a ... Demo. Text, 1.3.4 for NLP using NLTK????!!!!!! Full form of NLTK is Natural Language Toolkit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is, a ... Demo. Text, 1.3.4 for NLP using NLTK????!!!!!! Full form of NLTK is Natural Language Toolkit'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is,',\n",
       " 'a',\n",
       " '...',\n",
       " 'Demo.',\n",
       " 'Text,',\n",
       " '1.3.4',\n",
       " 'for',\n",
       " 'NLP',\n",
       " 'using',\n",
       " 'NLTK????!!!!!!',\n",
       " 'Full',\n",
       " 'form',\n",
       " 'of',\n",
       " 'NLTK',\n",
       " 'is',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Toolkit']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a demo text for nlp using nltk. full form of nltk is natural language toolkit\n"
     ]
    }
   ],
   "source": [
    "text = 'This is a Demo Text for NLP using NLTK. Full form of NLTK is Natural Language Toolkit'\n",
    "lower_text = text.lower()\n",
    "print(lower_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделить строку на отдельные слова  re.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'demo',\n",
       " 'text',\n",
       " 'for',\n",
       " 'nlp',\n",
       " 'using',\n",
       " 'nltk.',\n",
       " 'full',\n",
       " 'form',\n",
       " 'of',\n",
       " 'nltk',\n",
       " 'is',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'toolkit']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words = re.split(r' ', lower_text)\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'This is a Demo 20.20.10 Text for NLP using NLTK. Full form of NLTK is Natural Language Toolkit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a Demo 20',\n",
       " '20',\n",
       " '10 Text for NLP using NLTK',\n",
       " ' Full form of NLTK is Natural Language Toolkit']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split(sep='!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Подсчет частот слов TF, IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF (term frequency — частота слова) – отношение числа вхождений слова к общему числу слов документа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'natural': 0.058823529411764705,\n",
       " 'toolkit': 0.058823529411764705,\n",
       " 'full': 0.058823529411764705,\n",
       " 'for': 0.058823529411764705,\n",
       " 'text': 0.058823529411764705,\n",
       " 'this': 0.058823529411764705,\n",
       " 'nltk.': 0.058823529411764705,\n",
       " 'language': 0.058823529411764705,\n",
       " 'demo': 0.058823529411764705,\n",
       " 'a': 0.058823529411764705,\n",
       " 'nlp': 0.058823529411764705,\n",
       " 'is': 0.11764705882352941,\n",
       " 'using': 0.058823529411764705,\n",
       " 'nltk': 0.058823529411764705,\n",
       " 'form': 0.058823529411764705,\n",
       " 'of': 0.058823529411764705}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeTF(word_dict, bag_of_words):\n",
    "    tf_dict = {}\n",
    "    bag_of_words_count = len(bag_of_words)\n",
    "    for word, count in word_dict.items():\n",
    "        tf_dict[word] = count / float(bag_of_words_count)\n",
    "    return tf_dict\n",
    "\n",
    "unique_words = set(bag_of_words)\n",
    "\n",
    "num_of_words = dict.fromkeys(unique_words, 0)\n",
    "for word in bag_of_words:\n",
    "    num_of_words[word] += 1\n",
    "\n",
    "tf = computeTF(num_of_words, bag_of_words)\n",
    "tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDF (inverse document frequency — обратная частота документа) — инверсия частоты, с которой некоторое слово встречается в документах коллекции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'nltk is a basic nlp library'\n",
    "bag_of_words_2 = re.split(r' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'natural': 0.6931471805599453,\n",
       " 'toolkit': 0.6931471805599453,\n",
       " 'nltk.': 0.6931471805599453,\n",
       " 'text': 0.6931471805599453,\n",
       " 'this': 0.6931471805599453,\n",
       " 'language': 0.6931471805599453,\n",
       " 'demo': 0.6931471805599453,\n",
       " 'library': 0.6931471805599453,\n",
       " 'nltk': 0.0,\n",
       " 'is': 0.0,\n",
       " 'form': 0.6931471805599453,\n",
       " 'full': 0.6931471805599453,\n",
       " 'basic': 0.6931471805599453,\n",
       " 'for': 0.6931471805599453,\n",
       " 'a': 0.0,\n",
       " 'nlp': 0.0,\n",
       " 'using': 0.6931471805599453,\n",
       " 'of': 0.6931471805599453}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeIDF(documents):\n",
    "    import math\n",
    "    N = len(documents)\n",
    "    \n",
    "    idf_dict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idf_dict[word] += 1\n",
    "    \n",
    "    for word, val in idf_dict.items():\n",
    "        idf_dict[word] = math.log(N / float(val))\n",
    "    return idf_dict\n",
    "\n",
    "unique_words = set(bag_of_words).union(set(bag_of_words_2))\n",
    "\n",
    "num_of_words = dict.fromkeys(unique_words, 0)\n",
    "\n",
    "for word in bag_of_words:\n",
    "    num_of_words[word] += 1\n",
    "    \n",
    "num_of_words_2 = dict.fromkeys(unique_words, 0)\n",
    "\n",
    "for word in bag_of_words_2:\n",
    "    num_of_words_2[word] += 1\n",
    "\n",
    "idfs = computeIDF([num_of_words, num_of_words_2])\n",
    "idfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF (сокращение от term frequency — inverse document frequency) – это статистическая мера для оценки важности слова в документе, который является частью коллекции или корпуса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>aw</th>\n",
       "      <th>f</th>\n",
       "      <th>fu</th>\n",
       "      <th>h</th>\n",
       "      <th>ha</th>\n",
       "      <th>i</th>\n",
       "      <th>i</th>\n",
       "      <th>it</th>\n",
       "      <th>...</th>\n",
       "      <th>vie</th>\n",
       "      <th>w</th>\n",
       "      <th>wa</th>\n",
       "      <th>was</th>\n",
       "      <th>we</th>\n",
       "      <th>wes</th>\n",
       "      <th>y</th>\n",
       "      <th>y</th>\n",
       "      <th>y f</th>\n",
       "      <th>y.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 144 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       a   aw   f   fu   h   ha   i   i    it  ...  vie  w  wa  was  we  wes  \\\n",
       "0  6   0    0   2    2   0    0   1    0    1  ...    1  0   0    0   0    0   \n",
       "1  3   0    0   0    0   1    1   0    0    0  ...    1  0   0    0   0    0   \n",
       "2  5   1    1   0    0   0    0   2    1    1  ...    0  2   1    1   1    1   \n",
       "3  4   0    0   0    0   0    0   2    1    1  ...    0  0   0    0   0    0   \n",
       "\n",
       "   y  y   y f  y.  \n",
       "0  2   1    1   1  \n",
       "1  0   0    0   0  \n",
       "2  0   0    0   0  \n",
       "3  0   0    0   0  \n",
       "\n",
       "[4 rows x 144 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "document = [\"I like this movie, it's funny funny.\", 'I hate this movie.', 'This was awesome! I like it.', 'Nice one. I love it.']\n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(1, 3))\n",
    "values = vectorizer.fit_transform(document)\n",
    "\n",
    "# Show the Model as a pandas DataFrame\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "pd.DataFrame(values.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>awesome</th>\n",
       "      <th>funny</th>\n",
       "      <th>hate</th>\n",
       "      <th>it</th>\n",
       "      <th>like</th>\n",
       "      <th>love</th>\n",
       "      <th>movie</th>\n",
       "      <th>nice</th>\n",
       "      <th>one</th>\n",
       "      <th>this</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.571848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365003</td>\n",
       "      <td>0.450852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.450852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365003</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.702035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.553492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.539445</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.344321</td>\n",
       "      <td>0.425305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.344321</td>\n",
       "      <td>0.539445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.345783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    awesome     funny      hate        it      like      love     movie  \\\n",
       "0  0.000000  0.571848  0.000000  0.365003  0.450852  0.000000  0.450852   \n",
       "1  0.000000  0.000000  0.702035  0.000000  0.000000  0.000000  0.553492   \n",
       "2  0.539445  0.000000  0.000000  0.344321  0.425305  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.345783  0.000000  0.541736  0.000000   \n",
       "\n",
       "       nice       one      this       was  \n",
       "0  0.000000  0.000000  0.365003  0.000000  \n",
       "1  0.000000  0.000000  0.448100  0.000000  \n",
       "2  0.000000  0.000000  0.344321  0.539445  \n",
       "3  0.541736  0.541736  0.000000  0.000000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "document = [\"I like this movie, it's funny.\", 'I hate this movie.', 'This was awesome! I like it.', 'Nice one. I love it.']\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "values = tfidf_vectorizer.fit_transform(document)\n",
    "\n",
    "# Show the Model as a pandas DataFrame\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "pd.DataFrame(values.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Введение в библиотеку NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK (Natural Language Toolkit)\n",
    "Ведущая платформа для создания NLP-программ на Python. У нее есть легкие в использовании интерфейсы для многих языковых корпусов, а также библиотеки для обработки текстов для классификации, токенизации, стемминга, разметки и фильтрации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package nltk:\n",
      "\n",
      "NAME\n",
      "    nltk\n",
      "\n",
      "DESCRIPTION\n",
      "    The Natural Language Toolkit (NLTK) is an open source Python library\n",
      "    for Natural Language Processing.  A free online book is available.\n",
      "    (If you use the library for academic research, please cite the book.)\n",
      "    \n",
      "    Steven Bird, Ewan Klein, and Edward Loper (2009).\n",
      "    Natural Language Processing with Python.  O'Reilly Media Inc.\n",
      "    http://nltk.org/book\n",
      "    \n",
      "    @version: 3.4.5\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    app (package)\n",
      "    book\n",
      "    ccg (package)\n",
      "    chat (package)\n",
      "    chunk (package)\n",
      "    classify (package)\n",
      "    cluster (package)\n",
      "    collections\n",
      "    collocations\n",
      "    compat\n",
      "    corpus (package)\n",
      "    data\n",
      "    decorators\n",
      "    downloader\n",
      "    draw (package)\n",
      "    featstruct\n",
      "    grammar\n",
      "    help\n",
      "    inference (package)\n",
      "    internals\n",
      "    jsontags\n",
      "    lazyimport\n",
      "    lm (package)\n",
      "    metrics (package)\n",
      "    misc (package)\n",
      "    parse (package)\n",
      "    probability\n",
      "    sem (package)\n",
      "    sentiment (package)\n",
      "    stem (package)\n",
      "    tag (package)\n",
      "    tbl (package)\n",
      "    test (package)\n",
      "    text\n",
      "    tgrep\n",
      "    tokenize (package)\n",
      "    toolbox\n",
      "    translate (package)\n",
      "    tree\n",
      "    treeprettyprinter\n",
      "    treetransforms\n",
      "    twitter (package)\n",
      "    util\n",
      "    wsd\n",
      "\n",
      "SUBMODULES\n",
      "    agreement\n",
      "    aline\n",
      "    api\n",
      "    association\n",
      "    bleu_score\n",
      "    bllip\n",
      "    boxer\n",
      "    brill\n",
      "    brill_trainer\n",
      "    casual\n",
      "    chart\n",
      "    cistem\n",
      "    confusionmatrix\n",
      "    corenlp\n",
      "    crf\n",
      "    decisiontree\n",
      "    dependencygraph\n",
      "    discourse\n",
      "    distance\n",
      "    drt\n",
      "    earleychart\n",
      "    evaluate\n",
      "    featurechart\n",
      "    glue\n",
      "    hmm\n",
      "    hunpos\n",
      "    ibm1\n",
      "    ibm2\n",
      "    ibm3\n",
      "    ibm4\n",
      "    ibm5\n",
      "    ibm_model\n",
      "    isri\n",
      "    lancaster\n",
      "    lfg\n",
      "    linearlogic\n",
      "    logic\n",
      "    mace\n",
      "    malt\n",
      "    mapping\n",
      "    maxent\n",
      "    megam\n",
      "    meteor_score\n",
      "    mwe\n",
      "    naivebayes\n",
      "    nonprojectivedependencyparser\n",
      "    paice\n",
      "    pchart\n",
      "    perceptron\n",
      "    porter\n",
      "    positivenaivebayes\n",
      "    projectivedependencyparser\n",
      "    prover9\n",
      "    punkt\n",
      "    recursivedescent\n",
      "    regexp\n",
      "    relextract\n",
      "    repp\n",
      "    resolution\n",
      "    ribes_score\n",
      "    rslp\n",
      "    rte_classify\n",
      "    scikitlearn\n",
      "    scores\n",
      "    segmentation\n",
      "    senna\n",
      "    sequential\n",
      "    sexpr\n",
      "    shiftreduce\n",
      "    simple\n",
      "    snowball\n",
      "    sonority_sequencing\n",
      "    spearman\n",
      "    stack_decoder\n",
      "    stanford\n",
      "    stanford_segmenter\n",
      "    tableau\n",
      "    tadm\n",
      "    textcat\n",
      "    texttiling\n",
      "    tnt\n",
      "    toktok\n",
      "    transitionparser\n",
      "    treebank\n",
      "    viterbi\n",
      "    weka\n",
      "    wordnet\n",
      "\n",
      "FUNCTIONS\n",
      "    demo()\n",
      "        # FIXME:  override any accidentally imported demo, see https://github.com/nltk/nltk/issues/2116\n",
      "\n",
      "DATA\n",
      "    RUS_PICKLE = 'taggers/averaged_perceptron_tagger_ru/averaged_perceptro...\n",
      "    SLASH = *slash*\n",
      "    TYPE = *type*\n",
      "    __author_email__ = 'stevenbird1@gmail.com'\n",
      "    __classifiers__ = ['Development Status :: 5 - Production/Stable', 'Int...\n",
      "    __copyright__ = 'Copyright (C) 2001-2019 NLTK Project.\\n\\nDistribut......\n",
      "    __keywords__ = ['NLP', 'CL', 'natural language processing', 'computati...\n",
      "    __license__ = 'Apache License, Version 2.0'\n",
      "    __longdescr__ = 'The Natural Language Toolkit (NLTK) is a Python ... p...\n",
      "    __maintainer__ = 'Steven Bird, Edward Loper, Ewan Klein'\n",
      "    __maintainer_email__ = 'stevenbird1@gmail.com'\n",
      "    __url__ = 'http://nltk.org/'\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "    app = <LazyModule 'nltk.nltk.app'>\n",
      "    chat = <LazyModule 'nltk.nltk.chat'>\n",
      "    class_types = (<class 'type'>,)\n",
      "    corpus = <LazyModule 'nltk.nltk.corpus'>\n",
      "    improved_close_quote_regex = re.compile('([»”’])')\n",
      "    improved_open_quote_regex = re.compile('([«“‘„]|[`]+)')\n",
      "    improved_open_single_quote_regex = re.compile(\"(?i)(\\\\')(?!re|ve|ll|m|...\n",
      "    improved_punct_regex = re.compile('([^\\\\.])(\\\\.)([\\\\]\\\\)}>\"\\\\\\'»”’ ]*)...\n",
      "    infile = <_io.TextIOWrapper name='/home/rzaharov@mvs.loca...packages/n...\n",
      "    json_tags = {'!nltk.tag.BrillTagger': <class 'nltk.tag.brill.BrillTagg...\n",
      "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
      "    string_types = (<class 'str'>,)\n",
      "    toolbox = <LazyModule 'nltk.nltk.toolbox'>\n",
      "    version_file = '/home/rzaharov@mvs.local/.local/share/virtualenv...A3n...\n",
      "    version_info = sys.version_info(major=3, minor=8, micro=3, releaseleve...\n",
      "\n",
      "VERSION\n",
      "    3.4.5\n",
      "\n",
      "AUTHOR\n",
      "    Steven Bird, Edward Loper, Ewan Klein\n",
      "\n",
      "FILE\n",
      "    /home/rzaharov@mvs.local/.local/share/virtualenvs/nlp-course-A3nIyBIP/lib/python3.8/site-packages/nltk/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "help(nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Токенизация по предложениям \n",
    "– это процесс разделения письменного языка на предложения-компоненты. В английском и некоторых других языках мы можем вычленять предложение каждый раз, когда находим определенный знак пунктуации – точку.\n",
    "\n",
    "Но даже в английском эта задача нетривиальна, так как точка используется и в сокращениях. Таблица сокращений может сильно помочь во время обработки текста, чтобы избежать неверной расстановки границ предложений. В большинстве случаев для этого используются библиотеки, так что можете особо не переживать о деталях реализации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Russian is an East Slavic language, which is an official language in Russia, Belarus, Kazakhstan, Kyrgyzstan, as well as being widely used throughout Eastern Europe, the Baltic states, the Caucasus and Central Asia. Russian belongs to the family of Indo-European languages, one of the four living members of the East Slavic languages alongside, and part of the larger Balto-Slavic branch. There is a high degree of mutual intelligibility between Russian, Belarusian and Ukrainian.'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Russian is an East Slavic language, which is an official language in Russia, Belarus, Kazakhstan, Kyrgyzstan, as well as being widely used throughout Eastern Europe, the Baltic states, the Caucasus and Central Asia. Russian belongs to the family of Indo-European languages, one of the four living members of the East Slavic languages alongside, and part of the larger Balto-Slavic branch. There is a high degree of mutual intelligibility between Russian, Belarusian and Ukrainian.\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Russian',\n",
       " 'is',\n",
       " 'an',\n",
       " 'East',\n",
       " 'Slavic',\n",
       " 'language,',\n",
       " 'which',\n",
       " 'is',\n",
       " 'an',\n",
       " 'official',\n",
       " 'language',\n",
       " 'in',\n",
       " 'Russia,',\n",
       " 'Belarus,',\n",
       " 'Kazakhstan,',\n",
       " 'Kyrgyzstan,',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'being',\n",
       " 'widely',\n",
       " 'used',\n",
       " 'throughout',\n",
       " 'Eastern',\n",
       " 'Europe,',\n",
       " 'the',\n",
       " 'Baltic',\n",
       " 'states,',\n",
       " 'the',\n",
       " 'Caucasus',\n",
       " 'and',\n",
       " 'Central',\n",
       " 'Asia.',\n",
       " 'Russian',\n",
       " 'belongs',\n",
       " 'to',\n",
       " 'the',\n",
       " 'family',\n",
       " 'of',\n",
       " 'Indo-European',\n",
       " 'languages,',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'four',\n",
       " 'living',\n",
       " 'members',\n",
       " 'of',\n",
       " 'the',\n",
       " 'East',\n",
       " 'Slavic',\n",
       " 'languages',\n",
       " 'alongside,',\n",
       " 'and',\n",
       " 'part',\n",
       " 'of',\n",
       " 'the',\n",
       " 'larger',\n",
       " 'Balto-Slavic',\n",
       " 'branch.',\n",
       " 'There',\n",
       " 'is',\n",
       " 'a',\n",
       " 'high',\n",
       " 'degree',\n",
       " 'of',\n",
       " 'mutual',\n",
       " 'intelligibility',\n",
       " 'between',\n",
       " 'Russian,',\n",
       " 'Belarusian',\n",
       " 'and',\n",
       " 'Ukrainian.']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Russian is an East Slavic language, which is an official language in Russia, Belarus, Kazakhstan, Kyrgyzstan, as well as being widely used throughout Eastern Europe, the Baltic states, the Caucasus and Central Asia',\n",
       " ' Russian belongs to the family of Indo-European languages, one of the four living members of the East Slavic languages alongside, and part of the larger Balto-Slavic branch',\n",
       " ' There is a high degree of mutual intelligibility between Russian, Belarusian and Ukrainian',\n",
       " '']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize as tknz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =['Russian is an East Slavic language, which is an official language in Russia, Belarus, Kazakhstan, Kyrgyzstan, as well as being widely used throughout Eastern Europe, the Baltic states, the Caucasus and Central Asia',\n",
    " ' Russian belongs to the family of Indo-European languages, one of the four living members of the East Slavic languages alongside, and part of the larger Balto-Slavic branch',\n",
    " ' There is a high degree of mutual intelligibility between Russian, Belarusian and Ukrainian',\n",
    " '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\user/nltk_data'\n    - 'C:\\\\Users\\\\user\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-59b3b07796cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtknz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \"\"\"\n\u001b[1;32m--> 144\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m     return [\n\u001b[0;32m    146\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \"\"\"\n\u001b[1;32m--> 105\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 993\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    994\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\user/nltk_data'\n    - 'C:\\\\Users\\\\user\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "tknz.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Russian',\n",
       " 'is',\n",
       " 'an',\n",
       " 'East',\n",
       " 'Slavic',\n",
       " 'language',\n",
       " ',',\n",
       " 'which',\n",
       " 'is',\n",
       " 'an',\n",
       " 'official',\n",
       " 'language',\n",
       " 'in',\n",
       " 'Russia',\n",
       " ',',\n",
       " 'Belarus',\n",
       " ',',\n",
       " 'Kazakhstan',\n",
       " ',',\n",
       " 'Kyrgyzstan',\n",
       " ',',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'being',\n",
       " 'widely',\n",
       " 'used',\n",
       " 'throughout',\n",
       " 'Eastern',\n",
       " 'Europe',\n",
       " ',',\n",
       " 'the',\n",
       " 'Baltic',\n",
       " 'states',\n",
       " ',',\n",
       " 'the',\n",
       " 'Caucasus',\n",
       " 'and',\n",
       " 'Central',\n",
       " 'Asia',\n",
       " '.',\n",
       " 'Russian',\n",
       " 'belongs',\n",
       " 'to',\n",
       " 'the',\n",
       " 'family',\n",
       " 'of',\n",
       " 'Indo',\n",
       " '-',\n",
       " 'European',\n",
       " 'languages',\n",
       " ',',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'four',\n",
       " 'living',\n",
       " 'members',\n",
       " 'of',\n",
       " 'the',\n",
       " 'East',\n",
       " 'Slavic',\n",
       " 'languages',\n",
       " 'alongside',\n",
       " ',',\n",
       " 'and',\n",
       " 'part',\n",
       " 'of',\n",
       " 'the',\n",
       " 'larger',\n",
       " 'Balto',\n",
       " '-',\n",
       " 'Slavic',\n",
       " 'branch',\n",
       " '.',\n",
       " 'There',\n",
       " 'is',\n",
       " 'a',\n",
       " 'high',\n",
       " 'degree',\n",
       " 'of',\n",
       " 'mutual',\n",
       " 'intelligibility',\n",
       " 'between',\n",
       " 'Russian',\n",
       " ',',\n",
       " 'Belarusian',\n",
       " 'and',\n",
       " 'Ukrainian',\n",
       " '.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknz.wordpunct_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Russian',\n",
       " 'is',\n",
       " 'an',\n",
       " 'East',\n",
       " 'Slavic',\n",
       " 'language',\n",
       " ',',\n",
       " 'which',\n",
       " 'is',\n",
       " 'an',\n",
       " 'official',\n",
       " 'language',\n",
       " 'in',\n",
       " 'Russia',\n",
       " ',',\n",
       " 'Belarus',\n",
       " ',',\n",
       " 'Kazakhstan',\n",
       " ',',\n",
       " 'Kyrgyzstan',\n",
       " ',',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'being',\n",
       " 'widely',\n",
       " 'used',\n",
       " 'throughout',\n",
       " 'Eastern',\n",
       " 'Europe',\n",
       " ',',\n",
       " 'the',\n",
       " 'Baltic',\n",
       " 'states',\n",
       " ',',\n",
       " 'the',\n",
       " 'Caucasus',\n",
       " 'and',\n",
       " 'Central',\n",
       " 'Asia.',\n",
       " 'Russian',\n",
       " 'belongs',\n",
       " 'to',\n",
       " 'the',\n",
       " 'family',\n",
       " 'of',\n",
       " 'Indo-European',\n",
       " 'languages',\n",
       " ',',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'four',\n",
       " 'living',\n",
       " 'members',\n",
       " 'of',\n",
       " 'the',\n",
       " 'East',\n",
       " 'Slavic',\n",
       " 'languages',\n",
       " 'alongside',\n",
       " ',',\n",
       " 'and',\n",
       " 'part',\n",
       " 'of',\n",
       " 'the',\n",
       " 'larger',\n",
       " 'Balto-Slavic',\n",
       " 'branch.',\n",
       " 'There',\n",
       " 'is',\n",
       " 'a',\n",
       " 'high',\n",
       " 'degree',\n",
       " 'of',\n",
       " 'mutual',\n",
       " 'intelligibility',\n",
       " 'between',\n",
       " 'Russian',\n",
       " ',',\n",
       " 'Belarusian',\n",
       " 'and',\n",
       " 'Ukrainian',\n",
       " '.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknz.ToktokTokenizer().tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Russian',\n",
       " 'is',\n",
       " 'an',\n",
       " 'East',\n",
       " 'Slavic',\n",
       " 'language',\n",
       " ',',\n",
       " 'which',\n",
       " 'is',\n",
       " 'an',\n",
       " 'official',\n",
       " 'language',\n",
       " 'in',\n",
       " 'Russia',\n",
       " ',',\n",
       " 'Belarus',\n",
       " ',',\n",
       " 'Kazakhstan',\n",
       " ',',\n",
       " 'Kyrgyzstan',\n",
       " ',',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'being',\n",
       " 'widely',\n",
       " 'used',\n",
       " 'throughout',\n",
       " 'Eastern',\n",
       " 'Europe',\n",
       " ',',\n",
       " 'the',\n",
       " 'Baltic',\n",
       " 'states',\n",
       " ',',\n",
       " 'the',\n",
       " 'Caucasus',\n",
       " 'and',\n",
       " 'Central',\n",
       " 'Asia',\n",
       " '.',\n",
       " 'Russian',\n",
       " 'belongs',\n",
       " 'to',\n",
       " 'the',\n",
       " 'family',\n",
       " 'of',\n",
       " 'Indo-European',\n",
       " 'languages',\n",
       " ',',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'four',\n",
       " 'living',\n",
       " 'members',\n",
       " 'of',\n",
       " 'the',\n",
       " 'East',\n",
       " 'Slavic',\n",
       " 'languages',\n",
       " 'alongside',\n",
       " ',',\n",
       " 'and',\n",
       " 'part',\n",
       " 'of',\n",
       " 'the',\n",
       " 'larger',\n",
       " 'Balto-Slavic',\n",
       " 'branch',\n",
       " '.',\n",
       " 'There',\n",
       " 'is',\n",
       " 'a',\n",
       " 'high',\n",
       " 'degree',\n",
       " 'of',\n",
       " 'mutual',\n",
       " 'intelligibility',\n",
       " 'between',\n",
       " 'Russian',\n",
       " ',',\n",
       " 'Belarusian',\n",
       " 'and',\n",
       " 'Ukrainian',\n",
       " '.']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknz.TweetTokenizer().tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknz.RegexpTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Russian is an East 12.12.2020 Slavic language, which is an official language in Russia, Belarus, Kazakhstan, Kyrgyzstan, as well as being widely used throughout Eastern Europe, the Baltic states, the Caucasus and Central Asia. Russian belongs to the family of Indo-European languages, one of the four living members of the East Slavic languages alongside, and part of the larger Balto-Slavic branch. There is a high degree of mutual intelligibility between Russian, Belarusian and Ukrainian.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russian is an East 12.12.2020 Slavic language, which is an official language in Russia, Belarus, Kazakhstan, Kyrgyzstan, as well as being widely used throughout Eastern Europe, the Baltic states, the Caucasus and Central Asia.\n",
      "\n",
      "Russian belongs to the family of Indo-European languages, one of the four living members of the East Slavic languages alongside, and part of the larger Balto-Slavic branch.\n",
      "\n",
      "There is a high degree of mutual intelligibility between Russian, Belarusian and Ukrainian.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(text)\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Токенизация по словам \n",
    "– это процесс разделения предложений на слова-компоненты. В английском и многих других языках, использующих ту или иную версию латинского алфавита, пробел – это неплохой разделитель слов.\n",
    "\n",
    "Тем не менее, могут возникнуть проблемы, если мы будем использовать только пробел – в английском составные существительные пишутся по-разному и иногда через пробел. И тут вновь нам помогают библиотеки.\n",
    "\n",
    "Давайте возьмем предложения из предыдущего примера и применим к ним метод nltk.word_tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Russian is an East Slavic language, which is an official language in Russia, Belarus, Kazakhstan, Kyrgyzstan, as well as being widely used throughout Eastern Europe, the Baltic states, the Caucasus and Central Asia.',\n",
       " 'Russian belongs to the family of Indo-European languages, one of the four living members of the East Slavic languages alongside, and part of the larger Balto-Slavic branch.',\n",
       " 'There is a high degree of mutual intelligibility between Russian, Belarusian and Ukrainian.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = 'Russian is an East Slavic language. Ehich is 1.4.6 an official language in Russia. Belarus, Kazakhstan, Kyrgyzstan, as well as 10.22.2020 being widely used. throughout Eastern Europe, the Baltic states! The Caucasus and Central Asia.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.sent_tokenize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Russian is an East Slavic language.',\n",
       " 'Ehich is 1.4.6 an official language in Russia.',\n",
       " 'Belarus, Kazakhstan, Kyrgyzstan, as well as 10.22.2020 being widely used.',\n",
       " 'throughout Eastern Europe, the Baltic states!',\n",
       " 'The Caucasus and Central Asia.']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Выва аовлд 10.05.2020 аволдв. Иыты лдва олы!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Выва аовлд 10.05.2020 аволдв.', 'Иыты лдва олы!']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.sent_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Russian', 'is', 'an', 'East', 'Slavic', 'language', ',', 'which', 'is', 'an', 'official', 'language', 'in', 'Russia', ',', 'Belarus', ',', 'Kazakhstan', ',', 'Kyrgyzstan', ',', 'as', 'well', 'as', 'being', 'widely', 'used', 'throughout', 'Eastern', 'Europe', ',', 'the', 'Baltic', 'states', ',', 'the', 'Caucasus', 'and', 'Central', 'Asia', '.']\n",
      "\n",
      "['Russian', 'belongs', 'to', 'the', 'family', 'of', 'Indo-European', 'languages', ',', 'one', 'of', 'the', 'four', 'living', 'members', 'of', 'the', 'East', 'Slavic', 'languages', 'alongside', ',', 'and', 'part', 'of', 'the', 'larger', 'Balto-Slavic', 'branch', '.']\n",
      "\n",
      "['There', 'is', 'a', 'high', 'degree', 'of', 'mutual', 'intelligibility', 'between', 'Russian', ',', 'Belarusian', 'and', 'Ukrainian', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    print(words)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Лемматизация и стемминг текста\n",
    "\n",
    "Обычно тексты содержат разные грамматические формы одного и того же слова, а также могут встречаться однокоренные слова. Лемматизация и стемминг преследуют цель привести все встречающиеся словоформы к одной, нормальной словарной форме.\n",
    "\n",
    "##### Примеры:\n",
    "\n",
    "- Приведение разных словоформ к одной: dog, dogs, dog’s, dogs’ => dog\n",
    "- То же самое, но уже применительно к целому предложению: the boy’s dogs are different sizes => the boy dog be differ size\n",
    "\n",
    "\n",
    "Лемматизация и стемминг – это частные случаи нормализации и они отличаются.\n",
    "\n",
    "##### Стемминг\n",
    "– это грубый эвристический процесс, который отрезает «лишнее» от корня слов, часто это приводит к потере словообразовательных суффиксов.\n",
    "\n",
    "##### Лемматизация\n",
    "– это более тонкий процесс, который использует словарь и морфологический анализ, чтобы в итоге привести слово к его канонической форме – лемме.\n",
    "\n",
    "Отличие в том, что стеммер (конкретная реализация алгоритма стемминга – прим.переводчика) действует без знания контекста и, соответственно, не понимает разницу между словами, которые имеют разный смысл в зависимости от части речи. Однако у стеммеров есть и свои преимущества: их проще внедрить и они работают быстрее. Плюс, более низкая «аккуратность» может не иметь значения в некоторых случаях.\n",
    "\n",
    "##### Примеры:\n",
    "\n",
    "- Слово good – это лемма для слова better. Стеммер не увидит эту связь, так как здесь нужно сверяться со словарем.\n",
    "- Слово play – это базовая форма слова playing. Тут справятся и стемминг, и лемматизация.\n",
    "- Слово meeting может быть как нормальной формой существительного, так и формой глагола to meet, в зависимости от контекста. В - отличие от стемминга, лемматизация попробует выбрать правильную лемму, опираясь на контекст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SnowballStemmer.languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Практический опыт показывает, что реализация намеченного плана развития обеспечивает широкому кругу специалистов участие в формировании позиций, занимаемых участниками в отношении поставленных задач. Повседневная практика показывает, что новая модель организационной деятельности обеспечивает широкому кругу специалистов участие в формировании экономической целесообразности принимаемых решений. С другой стороны курс на социально-ориентированный национальный проект способствует подготовке и реализации модели развития!'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_test = \"Практический опыт показывает, что реализация намеченного плана развития обеспечивает широкому кругу специалистов участие в формировании позиций, занимаемых участниками в отношении поставленных задач. Повседневная практика показывает, что новая модель организационной деятельности обеспечивает широкому кругу специалистов участие в формировании экономической целесообразности принимаемых решений. С другой стороны курс на социально-ориентированный национальный проект способствует подготовке и реализации модели развития!\"\n",
    "text_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball = SnowballStemmer('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Практический',\n",
       " 'опыт',\n",
       " 'показывает',\n",
       " ',',\n",
       " 'что',\n",
       " 'реализация',\n",
       " 'намеченного',\n",
       " 'плана',\n",
       " 'развития',\n",
       " 'обеспечивает',\n",
       " 'широкому',\n",
       " 'кругу',\n",
       " 'специалистов',\n",
       " 'участие',\n",
       " 'в',\n",
       " 'формировании',\n",
       " 'позиций',\n",
       " ',',\n",
       " 'занимаемых',\n",
       " 'участниками',\n",
       " 'в',\n",
       " 'отношении',\n",
       " 'поставленных',\n",
       " 'задач',\n",
       " '.',\n",
       " 'Повседневная',\n",
       " 'практика',\n",
       " 'показывает',\n",
       " ',',\n",
       " 'что',\n",
       " 'новая',\n",
       " 'модель',\n",
       " 'организационной',\n",
       " 'деятельности',\n",
       " 'обеспечивает',\n",
       " 'широкому',\n",
       " 'кругу',\n",
       " 'специалистов',\n",
       " 'участие',\n",
       " 'в',\n",
       " 'формировании',\n",
       " 'экономической',\n",
       " 'целесообразности',\n",
       " 'принимаемых',\n",
       " 'решений',\n",
       " '.',\n",
       " 'С',\n",
       " 'другой',\n",
       " 'стороны',\n",
       " 'курс',\n",
       " 'на',\n",
       " 'социально-ориентированный',\n",
       " 'национальный',\n",
       " 'проект',\n",
       " 'способствует',\n",
       " 'подготовке',\n",
       " 'и',\n",
       " 'реализации',\n",
       " 'модели',\n",
       " 'развития',\n",
       " '!']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Практический - практическ\n",
      "опыт - оп\n",
      "показывает - показыва\n",
      ", - ,\n",
      "что - что\n",
      "реализация - реализац\n",
      "намеченного - намечен\n",
      "плана - план\n",
      "развития - развит\n",
      "обеспечивает - обеспечива\n",
      "широкому - широк\n",
      "кругу - круг\n",
      "специалистов - специалист\n",
      "участие - участ\n",
      "в - в\n",
      "формировании - формирован\n",
      "позиций - позиц\n",
      ", - ,\n",
      "занимаемых - занима\n",
      "участниками - участник\n",
      "в - в\n",
      "отношении - отношен\n",
      "поставленных - поставлен\n",
      "задач - задач\n",
      ". - .\n",
      "Повседневная - повседневн\n",
      "практика - практик\n",
      "показывает - показыва\n",
      ", - ,\n",
      "что - что\n",
      "новая - нов\n",
      "модель - модел\n",
      "организационной - организацион\n",
      "деятельности - деятельн\n",
      "обеспечивает - обеспечива\n",
      "широкому - широк\n",
      "кругу - круг\n",
      "специалистов - специалист\n",
      "участие - участ\n",
      "в - в\n",
      "формировании - формирован\n",
      "экономической - экономическ\n",
      "целесообразности - целесообразн\n",
      "принимаемых - принима\n",
      "решений - решен\n",
      ". - .\n",
      "С - с\n",
      "другой - друг\n",
      "стороны - сторон\n",
      "курс - курс\n",
      "на - на\n",
      "социально-ориентированный - социально-ориентирова\n",
      "национальный - национальн\n",
      "проект - проект\n",
      "способствует - способств\n",
      "подготовке - подготовк\n",
      "и - и\n",
      "реализации - реализац\n",
      "модели - модел\n",
      "развития - развит\n",
      "! - !\n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    print('{} - {}'.format(w, snowball.stem(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmer: seen\n",
      "Lemmatizer: see\n",
      "\n",
      "Stemmer: drove\n",
      "Lemmatizer: drive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word, pos):\n",
    "    \"\"\"\n",
    "    Print the results of stemmind and lemmitization using the passed stemmer, lemmatizer, word and pos (part of speech)\n",
    "    \"\"\"\n",
    "    print(\"Stemmer:\", stemmer.stem(word))\n",
    "    print(\"Lemmatizer:\", lemmatizer.lemmatize(word, pos))\n",
    "    print()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word = \"seen\", pos = wordnet.VERB)\n",
    "compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word = \"drove\", pos = wordnet.VERB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['практический', ' ', 'опыт', ' ', 'показывать', ' , ', 'что', ' ', 'реализация', ' ', 'намечать', ' ', 'план', ' ', 'развитие', ' ', 'обеспечивать', ' ', 'широкий', ' ', 'круг', ' ', 'специалист', ' ', 'участие', ' ', 'в', ' ', 'формирование', ' ', 'позиция', ' , ', 'занимать', ' ', 'участник', ' ', 'в', ' ', 'отношение', ' ', 'поставлять', ' ', 'задача', ' ', '. ', 'повседневный', ' ', 'практика', ' ', 'показывать', ' , ', 'что', ' ', 'новый', ' ', 'модель', ' ', 'организационный', ' ', 'деятельность', ' ', 'обеспечивать', ' ', 'широкий', ' ', 'круг', ' ', 'специалист', ' ', 'участие', ' ', 'в', ' ', 'формирование', ' ', 'экономический', ' ', 'целесообразность', ' ', 'принимать', ' ', 'решение', ' ', '. ', 'с', ' ', 'другой', ' ', 'сторона', ' ', 'курс', ' ', 'на', ' ', 'социально', '-', 'ориентированный', ' ', 'национальный', ' ', 'проект', ' ', 'способствовать', ' ', 'подготовка', ' ', 'и', ' ', 'реализация', ' ', 'модель', ' ', 'развитие', ' ', '!', '\\n']\n"
     ]
    }
   ],
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "m = Mystem()\n",
    "lemmas = m.lemmatize(' '.join(word))\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='стали', tag=OpencorporaTag('VERB,perf,intr plur,past,indc'), normal_form='стать', score=0.984662, methods_stack=((<DictionaryAnalyzer>, 'стали', 904, 4),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,gent'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 1),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,datv'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 2),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,loct'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 5),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn plur,nomn'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 6),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn plur,accs'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 9),))]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "morph.parse('стали')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Стоп-слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стоп-слова – это слова, которые выкидываются из текста до/после обработки текста. Когда мы применяем машинное обучение к текстам, такие слова могут добавить много шума, поэтому необходимо избавляться от нерелевантных слов.\n",
    "\n",
    "Стоп-слова это обычно понимают артикли, междометия, союзы и т.д., которые не несут смысловой нагрузки. При этом надо понимать, что не существует универсального списка стоп-слов, все зависит от конкретного случая.\n",
    "\n",
    "В NLTK есть предустановленный список стоп-слов. Перед первым использованием вам понадобится его скачать: nltk.download(“stopwords”). После скачивания можно импортировать пакет stopwords и посмотреть на сами слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words(\"russian\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим, как можно убрать стоп-слова из предложения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Backgammon', 'one', 'oldest', 'known', 'board', 'games', '.']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "sentence = \"Backgammon is one of the oldest known board games.\"\n",
    "\n",
    "words = nltk.word_tokenize(sentence)\n",
    "without_stop_words = [word for word in words if not word in stop_words]\n",
    "print(without_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = get_stop_words('ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['а',\n",
       " 'в',\n",
       " 'г',\n",
       " 'е',\n",
       " 'ж',\n",
       " 'и',\n",
       " 'к',\n",
       " 'м',\n",
       " 'о',\n",
       " 'с',\n",
       " 'т',\n",
       " 'у',\n",
       " 'я',\n",
       " 'бы',\n",
       " 'во',\n",
       " 'вы',\n",
       " 'да',\n",
       " 'до',\n",
       " 'ее',\n",
       " 'ей',\n",
       " 'ею',\n",
       " 'её',\n",
       " 'же',\n",
       " 'за',\n",
       " 'из',\n",
       " 'им',\n",
       " 'их',\n",
       " 'ли',\n",
       " 'мы',\n",
       " 'на',\n",
       " 'не',\n",
       " 'ни',\n",
       " 'но',\n",
       " 'ну',\n",
       " 'нх',\n",
       " 'об',\n",
       " 'он',\n",
       " 'от',\n",
       " 'по',\n",
       " 'со',\n",
       " 'та',\n",
       " 'те',\n",
       " 'то',\n",
       " 'ту',\n",
       " 'ты',\n",
       " 'уж',\n",
       " 'без',\n",
       " 'был',\n",
       " 'вам',\n",
       " 'вас',\n",
       " 'ваш',\n",
       " 'вон',\n",
       " 'вот',\n",
       " 'все',\n",
       " 'всю',\n",
       " 'вся',\n",
       " 'всё',\n",
       " 'где',\n",
       " 'год',\n",
       " 'два',\n",
       " 'две',\n",
       " 'дел',\n",
       " 'для',\n",
       " 'его',\n",
       " 'ему',\n",
       " 'еще',\n",
       " 'ещё',\n",
       " 'или',\n",
       " 'ими',\n",
       " 'имя',\n",
       " 'как',\n",
       " 'кем',\n",
       " 'ком',\n",
       " 'кто',\n",
       " 'лет',\n",
       " 'мне',\n",
       " 'мог',\n",
       " 'мож',\n",
       " 'мои',\n",
       " 'мой',\n",
       " 'мор',\n",
       " 'моя',\n",
       " 'моё',\n",
       " 'над',\n",
       " 'нам',\n",
       " 'нас',\n",
       " 'наш',\n",
       " 'нее',\n",
       " 'ней',\n",
       " 'нем',\n",
       " 'нет',\n",
       " 'нею',\n",
       " 'неё',\n",
       " 'них',\n",
       " 'оба',\n",
       " 'она',\n",
       " 'они',\n",
       " 'оно',\n",
       " 'под',\n",
       " 'пор',\n",
       " 'при',\n",
       " 'про',\n",
       " 'раз',\n",
       " 'сам',\n",
       " 'сих',\n",
       " 'так',\n",
       " 'там',\n",
       " 'тем',\n",
       " 'тех',\n",
       " 'том',\n",
       " 'тот',\n",
       " 'тою',\n",
       " 'три',\n",
       " 'тут',\n",
       " 'уже',\n",
       " 'чем',\n",
       " 'что',\n",
       " 'эта',\n",
       " 'эти',\n",
       " 'это',\n",
       " 'эту',\n",
       " 'алло',\n",
       " 'буду',\n",
       " 'будь',\n",
       " 'бывь',\n",
       " 'была',\n",
       " 'были',\n",
       " 'было',\n",
       " 'быть',\n",
       " 'вами',\n",
       " 'ваша',\n",
       " 'ваше',\n",
       " 'ваши',\n",
       " 'ведь',\n",
       " 'весь',\n",
       " 'вниз',\n",
       " 'всем',\n",
       " 'всех',\n",
       " 'всею',\n",
       " 'года',\n",
       " 'году',\n",
       " 'даже',\n",
       " 'двух',\n",
       " 'день',\n",
       " 'если',\n",
       " 'есть',\n",
       " 'зато',\n",
       " 'кого',\n",
       " 'кому',\n",
       " 'куда',\n",
       " 'лишь',\n",
       " 'люди',\n",
       " 'мало',\n",
       " 'меля',\n",
       " 'меня',\n",
       " 'мимо',\n",
       " 'мира',\n",
       " 'мной',\n",
       " 'мною',\n",
       " 'мочь',\n",
       " 'надо',\n",
       " 'нами',\n",
       " 'наша',\n",
       " 'наше',\n",
       " 'наши',\n",
       " 'него',\n",
       " 'нему',\n",
       " 'ниже',\n",
       " 'ними',\n",
       " 'один',\n",
       " 'пока',\n",
       " 'пора',\n",
       " 'пять',\n",
       " 'рано',\n",
       " 'сама',\n",
       " 'сами',\n",
       " 'само',\n",
       " 'саму',\n",
       " 'свое',\n",
       " 'свои',\n",
       " 'свою',\n",
       " 'себе',\n",
       " 'себя',\n",
       " 'семь',\n",
       " 'стал',\n",
       " 'суть',\n",
       " 'твой',\n",
       " 'твоя',\n",
       " 'твоё',\n",
       " 'тебе',\n",
       " 'тебя',\n",
       " 'теми',\n",
       " 'того',\n",
       " 'тоже',\n",
       " 'тому',\n",
       " 'туда',\n",
       " 'хоть',\n",
       " 'хотя',\n",
       " 'чаще',\n",
       " 'чего',\n",
       " 'чему',\n",
       " 'чтоб',\n",
       " 'чуть',\n",
       " 'этим',\n",
       " 'этих',\n",
       " 'этой',\n",
       " 'этом',\n",
       " 'этот',\n",
       " 'более',\n",
       " 'будем',\n",
       " 'будет',\n",
       " 'будто',\n",
       " 'будут',\n",
       " 'вверх',\n",
       " 'вдали',\n",
       " 'вдруг',\n",
       " 'везде',\n",
       " 'внизу',\n",
       " 'время',\n",
       " 'всего',\n",
       " 'всеми',\n",
       " 'всему',\n",
       " 'всюду',\n",
       " 'давно',\n",
       " 'даром',\n",
       " 'долго',\n",
       " 'друго',\n",
       " 'жизнь',\n",
       " 'занят',\n",
       " 'затем',\n",
       " 'зачем',\n",
       " 'здесь',\n",
       " 'иметь',\n",
       " 'какая',\n",
       " 'какой',\n",
       " 'когда',\n",
       " 'кроме',\n",
       " 'лучше',\n",
       " 'между',\n",
       " 'менее',\n",
       " 'много',\n",
       " 'могут',\n",
       " 'может',\n",
       " 'можно',\n",
       " 'можхо',\n",
       " 'назад',\n",
       " 'низко',\n",
       " 'нужно',\n",
       " 'одной',\n",
       " 'около',\n",
       " 'опять',\n",
       " 'очень',\n",
       " 'перед',\n",
       " 'позже',\n",
       " 'после',\n",
       " 'потом',\n",
       " 'почти',\n",
       " 'пятый',\n",
       " 'разве',\n",
       " 'рядом',\n",
       " 'самим',\n",
       " 'самих',\n",
       " 'самой',\n",
       " 'самом',\n",
       " 'своей',\n",
       " 'своих',\n",
       " 'сеаой',\n",
       " 'снова',\n",
       " 'собой',\n",
       " 'собою',\n",
       " 'такая',\n",
       " 'также',\n",
       " 'такие',\n",
       " 'такое',\n",
       " 'такой',\n",
       " 'тобой',\n",
       " 'тобою',\n",
       " 'тогда',\n",
       " 'тысяч',\n",
       " 'уметь',\n",
       " 'часто',\n",
       " 'через',\n",
       " 'чтобы',\n",
       " 'шесть',\n",
       " 'этими',\n",
       " 'этого',\n",
       " 'этому',\n",
       " 'близко',\n",
       " 'больше',\n",
       " 'будете',\n",
       " 'будешь',\n",
       " 'бывает',\n",
       " 'важная',\n",
       " 'важное',\n",
       " 'важные',\n",
       " 'важный',\n",
       " 'вокруг',\n",
       " 'восемь',\n",
       " 'всегда',\n",
       " 'второй',\n",
       " 'далеко',\n",
       " 'дальше',\n",
       " 'девять',\n",
       " 'десять',\n",
       " 'должно',\n",
       " 'другая',\n",
       " 'другие',\n",
       " 'других',\n",
       " 'другое',\n",
       " 'другой',\n",
       " 'занята',\n",
       " 'занято',\n",
       " 'заняты',\n",
       " 'значит',\n",
       " 'именно',\n",
       " 'иногда',\n",
       " 'каждая',\n",
       " 'каждое',\n",
       " 'каждые',\n",
       " 'каждый',\n",
       " 'кругом',\n",
       " 'меньше',\n",
       " 'начала',\n",
       " 'нельзя',\n",
       " 'нибудь',\n",
       " 'никуда',\n",
       " 'ничего',\n",
       " 'обычно',\n",
       " 'однако',\n",
       " 'одного',\n",
       " 'отсюда',\n",
       " 'первый',\n",
       " 'потому',\n",
       " 'почему',\n",
       " 'просто',\n",
       " 'против',\n",
       " 'раньше',\n",
       " 'самими',\n",
       " 'самого',\n",
       " 'самому',\n",
       " 'своего',\n",
       " 'сейчас',\n",
       " 'сказал',\n",
       " 'совсем',\n",
       " 'теперь',\n",
       " 'только',\n",
       " 'третий',\n",
       " 'хорошо',\n",
       " 'хотеть',\n",
       " 'хочешь',\n",
       " 'четыре',\n",
       " 'шестой',\n",
       " 'восьмой',\n",
       " 'впрочем',\n",
       " 'времени',\n",
       " 'говорил',\n",
       " 'говорит',\n",
       " 'девятый',\n",
       " 'десятый',\n",
       " 'кажется',\n",
       " 'конечно',\n",
       " 'которая',\n",
       " 'которой',\n",
       " 'которые',\n",
       " 'который',\n",
       " 'которых',\n",
       " 'наверху',\n",
       " 'наконец',\n",
       " 'недавно',\n",
       " 'немного',\n",
       " 'нередко',\n",
       " 'никогда',\n",
       " 'однажды',\n",
       " 'посреди',\n",
       " 'сегодня',\n",
       " 'седьмой',\n",
       " 'сказала',\n",
       " 'сказать',\n",
       " 'сколько',\n",
       " 'слишком',\n",
       " 'сначала',\n",
       " 'спасибо',\n",
       " 'человек',\n",
       " 'двадцать',\n",
       " 'довольно',\n",
       " 'которого',\n",
       " 'наиболее',\n",
       " 'недалеко',\n",
       " 'особенно',\n",
       " 'отовсюду',\n",
       " 'двадцатый',\n",
       " 'миллионов',\n",
       " 'несколько',\n",
       " 'прекрасно',\n",
       " 'процентов',\n",
       " 'четвертый',\n",
       " 'двенадцать',\n",
       " 'непрерывно',\n",
       " 'пожалуйста',\n",
       " 'пятнадцать',\n",
       " 'семнадцать',\n",
       " 'тринадцать',\n",
       " 'двенадцатый',\n",
       " 'одиннадцать',\n",
       " 'пятнадцатый',\n",
       " 'семнадцатый',\n",
       " 'тринадцатый',\n",
       " 'шестнадцать',\n",
       " 'восемнадцать',\n",
       " 'девятнадцать',\n",
       " 'одиннадцатый',\n",
       " 'четырнадцать',\n",
       " 'шестнадцатый',\n",
       " 'восемнадцатый',\n",
       " 'девятнадцатый',\n",
       " 'действительно',\n",
       " 'четырнадцатый',\n",
       " 'многочисленная',\n",
       " 'многочисленное',\n",
       " 'многочисленные',\n",
       " 'многочисленный']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сделаем стандартную предобработку данных с сайта Lenta.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34679</th>\n",
       "      <td>Белоруссия отпустит курс рубля</td>\n",
       "      <td>Белоруссия отпустит курс национальной валюты с середины сентября. Об этом заявил президент страны Александр Лукашенко, передает \"Интерфакс\". Как передает БЕЛТА, президент добавил, что курс белорусского рубля будет определяться спросом и предложением. Рыночный курс национальной валюты будет устанавливаться на дополнительной сессии торгов на валютной бирже. \"Мы искусственно поддерживать курс не планируем\", - цитирует Лукашенко \"Интерфакс\". РИА Новости указывает, что одновременно Лукашенко поручил комитету государственного контроля воспрепятствовать спекуляциям, которые может вызвать ситуация в стране. Президент добавил, что Нацбанк Белоруссии \"должен ежедневно отслеживать ситуацию и при необходимости вмешиваться, сбивая спекулятивные атаки\". Сейчас в Белоруссии, с весны 2011 года пережив...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38792</th>\n",
       "      <td>Saxo Bank предсказал закрытие бирж и банков в Европе на неделю</td>\n",
       "      <td>Saxo Bank опубликовал свои ежегодные \"Шокирующие предсказания\" для мировой экономики на будущий год. Среди рисков, которые инвесторам стоит учитывать при формировании портфеля в 2012 году, Saxo Bank назвал рецессию в Австралии, падение акций Apple в два раза и закрытие банков и бирж в Европе на неделю и более. В своих предсказаниях Saxo Bank описывает маловероятные события, возможность которых, тем не менее, является выше ожиданий инвесторов. Если что-то из описанного произойдет, то на рынки будет оказано очень сильное давление. В десятку возможных событий вошли также победа нового кандидата в борьбе на выборах президента США, введение правил банковского регулирования \"Базель-3\", которое может привести к национализации 50 банков в ЕС, замена Швейцарии Швецией и Норвегией в качестве мир...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37432</th>\n",
       "      <td>На Бориса Гребенщикова напали в центре Киева</td>\n",
       "      <td>На лидера группы «Аквариум» Бориса Гребенщикова во время уличного концерта в Киеве напал неизвестный. Об этом сообщает издание «Украина.ру». Отмечается, что на сцену прорвался национал-радикал, который пытался напасть на артиста. Нападавший обвинил музыканта в «рашизме», однако был оперативно задержан. Гребенщиков пожелал провокатору прийти в себя. Рок-музыкант прибыл в Киев 22 июня, чтобы провести ряд концертов в украинской столице.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22752</th>\n",
       "      <td>Центробанк потребовал ужесточить подход к работе с VIP-клиентами</td>\n",
       "      <td>Банк России потребовал от финансово-кредитных организаций ужесточить подход к работе с клиентами премиального сегмента. Об этом в понедельник, 14 марта, сообщает «Коммерсантъ» со ссылкой на письмо регулятора, направленное банкирам в начале текущего месяца. ЦБ связывает такое требование с активизировавшейся в последнее время схемой по обналичиванию средств VIP-клиентов через их карты. Регулятор обратил внимание на объемы сомнительных наличных операций, которые сохраняются «на высоком уровне». Они проводятся банками прежде всего через карты физических лиц премиального и массового сегментов. Схема заключается в следующем: транзитные компании, аккумулирующие деньги на своих счетах, распределяют средства на карты физлиц суммами от 100 тысяч до 3 миллионов рублей в течение 1-3 месяцев под ви...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18749</th>\n",
       "      <td>Футболист \"Челси\" признался в употреблении кокаина</td>\n",
       "      <td>Нападающий \"Челси\" и сборной Румынии Адриан Муту признался в употреблении кокаина. Об этом сообщил глава профсоюза британских футболистов Гордон Тейлор. Он отметил, что Муту не будет ждать результатов допинг-пробы \"В\" и теперь ждет наказания, которая вынесет футбольная ассоциация Англии. \"Надеюсь, что ФА вынесет свое решение как можно быстрее на основании тех данных, которые у них есть\", - сказал Тейлор. Муту может быть дисквалифицирован на два года, но существует вероятность, что наказание будет менее строгим, с учетом признания самого футболиста. Адриан Муту в настоящее время наказан и своим клубом \"Челси\", который оставил игрока без зарплаты до тех пор, пока футбольная ассоциация не огласит свое решение. Для игрока могут настать трудные дни, так как ему нужно платить заработную плат...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             text\n",
       "34679                                    Белоруссия отпустит курс рубля  Белоруссия отпустит курс национальной валюты с середины сентября. Об этом заявил президент страны Александр Лукашенко, передает \"Интерфакс\". Как передает БЕЛТА, президент добавил, что курс белорусского рубля будет определяться спросом и предложением. Рыночный курс национальной валюты будет устанавливаться на дополнительной сессии торгов на валютной бирже. \"Мы искусственно поддерживать курс не планируем\", - цитирует Лукашенко \"Интерфакс\". РИА Новости указывает, что одновременно Лукашенко поручил комитету государственного контроля воспрепятствовать спекуляциям, которые может вызвать ситуация в стране. Президент добавил, что Нацбанк Белоруссии \"должен ежедневно отслеживать ситуацию и при необходимости вмешиваться, сбивая спекулятивные атаки\". Сейчас в Белоруссии, с весны 2011 года пережив...\n",
       "38792    Saxo Bank предсказал закрытие бирж и банков в Европе на неделю  Saxo Bank опубликовал свои ежегодные \"Шокирующие предсказания\" для мировой экономики на будущий год. Среди рисков, которые инвесторам стоит учитывать при формировании портфеля в 2012 году, Saxo Bank назвал рецессию в Австралии, падение акций Apple в два раза и закрытие банков и бирж в Европе на неделю и более. В своих предсказаниях Saxo Bank описывает маловероятные события, возможность которых, тем не менее, является выше ожиданий инвесторов. Если что-то из описанного произойдет, то на рынки будет оказано очень сильное давление. В десятку возможных событий вошли также победа нового кандидата в борьбе на выборах президента США, введение правил банковского регулирования \"Базель-3\", которое может привести к национализации 50 банков в ЕС, замена Швейцарии Швецией и Норвегией в качестве мир...\n",
       "37432                      На Бориса Гребенщикова напали в центре Киева                                                                                                                                                                                                                                                                                                                                                                            На лидера группы «Аквариум» Бориса Гребенщикова во время уличного концерта в Киеве напал неизвестный. Об этом сообщает издание «Украина.ру». Отмечается, что на сцену прорвался национал-радикал, который пытался напасть на артиста. Нападавший обвинил музыканта в «рашизме», однако был оперативно задержан. Гребенщиков пожелал провокатору прийти в себя. Рок-музыкант прибыл в Киев 22 июня, чтобы провести ряд концертов в украинской столице.\n",
       "22752  Центробанк потребовал ужесточить подход к работе с VIP-клиентами  Банк России потребовал от финансово-кредитных организаций ужесточить подход к работе с клиентами премиального сегмента. Об этом в понедельник, 14 марта, сообщает «Коммерсантъ» со ссылкой на письмо регулятора, направленное банкирам в начале текущего месяца. ЦБ связывает такое требование с активизировавшейся в последнее время схемой по обналичиванию средств VIP-клиентов через их карты. Регулятор обратил внимание на объемы сомнительных наличных операций, которые сохраняются «на высоком уровне». Они проводятся банками прежде всего через карты физических лиц премиального и массового сегментов. Схема заключается в следующем: транзитные компании, аккумулирующие деньги на своих счетах, распределяют средства на карты физлиц суммами от 100 тысяч до 3 миллионов рублей в течение 1-3 месяцев под ви...\n",
       "18749                Футболист \"Челси\" признался в употреблении кокаина  Нападающий \"Челси\" и сборной Румынии Адриан Муту признался в употреблении кокаина. Об этом сообщил глава профсоюза британских футболистов Гордон Тейлор. Он отметил, что Муту не будет ждать результатов допинг-пробы \"В\" и теперь ждет наказания, которая вынесет футбольная ассоциация Англии. \"Надеюсь, что ФА вынесет свое решение как можно быстрее на основании тех данных, которые у них есть\", - сказал Тейлор. Муту может быть дисквалифицирован на два года, но существует вероятность, что наказание будет менее строгим, с учетом признания самого футболиста. Адриан Муту в настоящее время наказан и своим клубом \"Челси\", который оставил игрока без зарплаты до тех пор, пока футбольная ассоциация не огласит свое решение. Для игрока могут настать трудные дни, так как ему нужно платить заработную плат..."
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', 800)\n",
    "\n",
    "data = pd.read_csv('lenta-ru-partial.csv')\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MorphAnalyzer()\n",
    "\n",
    "# убираем все небуквенные символы\n",
    "regex = re.compile(\"[А-Яа-яA-z]+\")\n",
    "\n",
    "def words_only(text, regex=regex):\n",
    "    try:\n",
    "        return regex.findall(text.lower())\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В южноафриканском Кейптауне победой сборной России завершился чемпионат мира среди бездомных. В финальном матче российские футболисты, впервые в своей истории ставшие чемпионами мира, обыграли команду Казахстана со счетом 1:0, передает BBC News. В первенстве принимали участие почти 500 человек, которые представляли 48 стран мира. Все матчи, каждый из которых продолжался 15 минут, проходили на асфальтовых полях, причем в одной команде могли играть как мужчины, так и женщины. Сборная России провела на турнире 13 матчей, во всех из которых добилась победы. На предыдущих чемпионатах мира достижения российской команды были скромнее: в 2003-м году – 13-е место, в 2004-м году – 5-е место, в 2005-м году – 12-е место.\n"
     ]
    }
   ],
   "source": [
    "print(data.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "в южноафриканском кейптауне победой сборной россии завершился чемпионат мира среди бездомных в финальном матче российские футболисты впервые в своей истории ставшие чемпионами мира обыграли команду казахстана со счетом передает bbc news в первенстве принимали участие почти человек которые представляли стран мира все матчи каждый из которых продолжался минут проходили на асфальтовых полях причем в одной команде могли играть как мужчины так и женщины сборная россии провела на турнире матчей во всех из которых добилась победы на предыдущих чемпионатах мира достижения российской команды были скромнее в м году е место в м году е место в м году е место\n"
     ]
    }
   ],
   "source": [
    "print(*words_only(data.text[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод @lru_cashe создает для функции lemmatize кэш указанного размера, что позволяет в целом ускорить лемматизацию текста (что очень полезно, так как лемматизация - ресурсоемкий процесс)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=128)\n",
    "def lemmatize_word(token, pymorphy=m):\n",
    "    return pymorphy.parse(token)[0].normal_form\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatize_word(w) for w in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['в', 'южноафриканский', 'кейптаун', 'победа', 'сборный', 'россия', 'завершиться', 'чемпионат', 'мир', 'среди', 'бездомный', 'в', 'финальный', 'матч', 'российский', 'футболист', 'впервые', 'в', 'свой', 'история', 'стать', 'чемпион', 'мир', 'обыграть', 'команда', 'казахстан', 'с', 'счёт', 'передавать', 'bbc', 'news', 'в', 'первенство', 'принимать', 'участие', 'почти', 'человек', 'который', 'представлять', 'страна', 'мир', 'весь', 'матч', 'каждый', 'из', 'который', 'продолжаться', 'минута', 'проходить', 'на', 'асфальтовый', 'поле', 'причём', 'в', 'один', 'команда', 'мочь', 'играть', 'как', 'мужчина', 'так', 'и', 'женщина', 'сборная', 'россия', 'провести', 'на', 'турнир', 'матч', 'в', 'весь', 'из', 'который', 'добиться', 'победа', 'на', 'предыдущий', 'чемпионат', 'мир', 'достижение', 'российский', 'команда', 'быть', 'скромный', 'в', 'метр', 'год', 'е', 'место', 'в', 'метр', 'год', 'е', 'место', 'в', 'метр', 'год', 'е', 'место']\n"
     ]
    }
   ],
   "source": [
    "tokens = words_only(data.text[0])\n",
    "\n",
    "print(lemmatize_text(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystopwords = stopwords.words('russian') \n",
    "\n",
    "def remove_stopwords(lemmas, stopwords = mystopwords):\n",
    "    return [w for w in lemmas if not w in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "южноафриканский кейптаун победа сборный россия завершиться чемпионат мир среди бездомный финальный матч российский футболист впервые свой история стать чемпион мир обыграть команда казахстан счёт передавать bbc news первенство принимать участие человек который представлять страна мир весь матч каждый который продолжаться минута проходить асфальтовый поле причём команда мочь играть мужчина женщина сборная россия провести турнир матч весь который добиться победа предыдущий чемпионат мир достижение российский команда скромный метр год е место метр год е место метр год е место\n"
     ]
    }
   ],
   "source": [
    "lemmas = lemmatize_text(tokens)\n",
    "\n",
    "print(*remove_stopwords(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(lemmas, stopwords = mystopwords):\n",
    "    return [w for w in lemmas if not w in stopwords and len(w) > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "южноафриканский кейптаун победа сборный россия завершиться чемпионат среди бездомный финальный матч российский футболист впервые свой история стать чемпион обыграть команда казахстан счёт передавать news первенство принимать участие человек который представлять страна весь матч каждый который продолжаться минута проходить асфальтовый поле причём команда мочь играть мужчина женщина сборная россия провести турнир матч весь который добиться победа предыдущий чемпионат достижение российский команда скромный метр место метр место метр место\n"
     ]
    }
   ],
   "source": [
    "print(*remove_stopwords(lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если собрать все в одну функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    tokens = words_only(text)\n",
    "    lemmas = lemmatize_text(tokens)\n",
    "    \n",
    "    return remove_stopwords(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "известный голливудский актёр майкл дуглас совершить неожиданный визит сообщать издание cubadebate цель поездка дуглас уточняться утверждаться лишь актёр посетить несколько памятный место число закусочный floridita который некогда любить бывать эрнест хемингуэй майкл дуглас также осмотреть достопримечательность исторический центр гавана понаблюдать процесс изготовление кубинский сигара табачный фабрика стоить отметить свободный посещение куба американский гражданин иметь родственник остров запретить американец поездка требоваться специальный разрешение государственный департамент получать дуглас разрешение сообщаться напомнить дуглас единственный знаменитый голливудский актёр посетить последний время ранее страна качество корреспондент журнал vanity fair прибыть визит пенна двукратный обладатель премия оскар намереваться взять интервью фидель кастро поездка пенный сопровождать известный филантроп дайана дженкинс\n"
     ]
    }
   ],
   "source": [
    "print(*clean_text(data.text[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если нужно предобработать большой объем текста, помимо кэширования может помочь распараллеливание, например, методом Pool библиотеки multiprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-95-53bf18337362>:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  lemmas = list(tqdm(p.imap(clean_text, data['text'][:N]), total=N))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a36fe1594046c29ffab77b8684c815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "N = 200\n",
    "\n",
    "with Pool(8) as p:\n",
    "    lemmas = list(tqdm(p.imap(clean_text, data['text'][:N]), total=N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>СМИ сообщили о госпитализации Армена Джигарханяна</td>\n",
       "      <td>Актер Армен Джигарханян доставлен в одну из столичных клиник и помещен в блок интенсивной терапии, сообщает 11 февраля LifeNews. Причиной госпитализации артиста стали жалобы на сердце. По словам врачей, состояние Джигарханяна средней тяжести. У 80-летнего актера ишемическая болезнь сердца, на фоне которой возникла нестабильная стенокардия. В клинику Джигарханяна доставила его 36-летняя супруга Виталина Цымбалюк-Романовская. Армен Джигарханян — народный артист СССР, актер и театральный режиссер, основатель Московского драматического театра под руководством Армена Джигарханяна. В его фильмографии такие картины, как «Здравствуйте, я ваша тетя!», «Место встречи изменить нельзя», «Мужчины», «Четвертый», «Профессия — следователь» и другие.</td>\n",
       "      <td>[актёр, армен, джигарханян, доставить, столичный, клиника, поместить, блок, интенсивный, терапия, сообщать, февраль, lifenews, причина, госпитализация, артист, стать, жалоба, сердце, слово, врач, состояние, джигарханян, средний, тяжесть, летний, актёр, ишемический, болезнь, сердце, который, возникнуть, нестабильный, стенокардия, клиника, джигарханян, доставить, летний, супруг, виталина, цымбалюк, романовский, армен, джигарханян, народный, артист, ссср, актёр, театральный, режиссёр, основатель, московский, драматический, театр, руководство, армен, джигарханян, фильмография, картина, здравствовать, тётя, место, встреча, изменить, мужчина, четыре, профессия, следователь]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Осязание роботов превзошло человеческое</td>\n",
       "      <td>Ученые разработали метод осязания, позволяющий роботам различать сходные по текстуре поверхности лучше, чем это делают люди. Работа опубликована в журнале Frontiers in Neurorobotics, ее краткое содержание можно прочитать на сайте Университета Южной калифорнии. Исследователи собрали тестового робота из коммерчески доступных компонентов и занялись созданием программного обеспечения, которое позволило бы ему максимально достоверно различать сходные поверхности. Тактильный сенсор, на основе которого происходило осязание, имитировал строение человеческого пальца. Снаружи он был покрыт эластичной \"кожей\", несущей папиллярные узоры, а внутри был заполнен жидкостью. Внутри находились датчики тепла, силы и давления. Когда искусственный палец скользил по поверхности, в жидкости возникали колебан...</td>\n",
       "      <td>[учёный, разработать, метод, осязание, позволять, робот, различать, сходный, текстура, поверхность, хороший, делать, человек, работа, опубликовать, журнал, frontiers, neurorobotics, краткий, содержание, прочитать, сайт, университет, южный, калифорния, исследователь, собрать, тестовый, робот, коммерчески, доступный, компонент, заняться, создание, программный, обеспечение, который, позволить, максимально, достоверно, различать, сходный, поверхность, тактильный, сенсор, основа, который, происходить, осязание, имитировать, строение, человеческий, палец, снаружи, покрыть, эластичный, кожа, несущий, папиллярный, узор, внутри, заполнить, жидкость, внутри, находиться, датчик, тепло, сила, давление, искусственный, палец, скользить, поверхность, жидкость, возникать, колебание, который, записыват...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Дель Боске продлил контракт со сборной Испании</td>\n",
       "      <td>Главный тренер сборной Испании по футболу Висенте дель Боске продлил соглашение с командой. Новый контракт будет действовать до конца чемпионата мира 2014 года, который пройдет в Бразилии, сообщает испанское издание Marca. \"Я вновь с испанской командой\", - приводит слова дель Боске Marca. Предыдущее соглашение дель Боске со сборной Испании заканчивалось после чемпионата Европы-2012. Дель Боске возглавил сборную Испании после победного чемпионата Европы-2008, заменив на этом посту Луиса Арагонеса. В 2010 году сборная Испании под руководством дель Боске впервые выиграла чемпионат мира. В финале мирового первенства, которое проходило в ЮАР, испанцы обыграли в дополнительное время сборную Нидерландов. Испанцы уверенно отобрались на чемпионат Европы-2012, который пройдет в Польше и Украине ...</td>\n",
       "      <td>[главный, тренер, сборный, испания, футбол, висенте, дель, боска, продлить, соглашение, команда, новый, контракт, действовать, конец, чемпионат, который, пройти, бразилия, сообщать, испанский, издание, marca, вновь, испанский, команда, приводить, слово, дель, боска, marca, предыдущий, соглашение, дель, боска, сборный, испания, заканчиваться, чемпионат, европа, дель, боска, возглавить, сборная, испания, победный, чемпионат, европа, заменить, пост, луис, арагонес, сборная, испания, руководство, дель, боска, впервые, выиграть, чемпионат, финал, мировой, первенство, который, проходить, испанец, обыграть, дополнительный, время, сборная, нидерланды, испанец, уверенно, отобраться, чемпионат, европа, который, пройти, польша, украина, июнь, июль, групповой, этап, турнир, испания, сыграть, итали...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           lemmas\n",
       "83  СМИ сообщили о госпитализации Армена Джигарханяна                                                          Актер Армен Джигарханян доставлен в одну из столичных клиник и помещен в блок интенсивной терапии, сообщает 11 февраля LifeNews. Причиной госпитализации артиста стали жалобы на сердце. По словам врачей, состояние Джигарханяна средней тяжести. У 80-летнего актера ишемическая болезнь сердца, на фоне которой возникла нестабильная стенокардия. В клинику Джигарханяна доставила его 36-летняя супруга Виталина Цымбалюк-Романовская. Армен Джигарханян — народный артист СССР, актер и театральный режиссер, основатель Московского драматического театра под руководством Армена Джигарханяна. В его фильмографии такие картины, как «Здравствуйте, я ваша тетя!», «Место встречи изменить нельзя», «Мужчины», «Четвертый», «Профессия — следователь» и другие.                                                                                                                             [актёр, армен, джигарханян, доставить, столичный, клиника, поместить, блок, интенсивный, терапия, сообщать, февраль, lifenews, причина, госпитализация, артист, стать, жалоба, сердце, слово, врач, состояние, джигарханян, средний, тяжесть, летний, актёр, ишемический, болезнь, сердце, который, возникнуть, нестабильный, стенокардия, клиника, джигарханян, доставить, летний, супруг, виталина, цымбалюк, романовский, армен, джигарханян, народный, артист, ссср, актёр, театральный, режиссёр, основатель, московский, драматический, театр, руководство, армен, джигарханян, фильмография, картина, здравствовать, тётя, место, встреча, изменить, мужчина, четыре, профессия, следователь]\n",
       "93            Осязание роботов превзошло человеческое  Ученые разработали метод осязания, позволяющий роботам различать сходные по текстуре поверхности лучше, чем это делают люди. Работа опубликована в журнале Frontiers in Neurorobotics, ее краткое содержание можно прочитать на сайте Университета Южной калифорнии. Исследователи собрали тестового робота из коммерчески доступных компонентов и занялись созданием программного обеспечения, которое позволило бы ему максимально достоверно различать сходные поверхности. Тактильный сенсор, на основе которого происходило осязание, имитировал строение человеческого пальца. Снаружи он был покрыт эластичной \"кожей\", несущей папиллярные узоры, а внутри был заполнен жидкостью. Внутри находились датчики тепла, силы и давления. Когда искусственный палец скользил по поверхности, в жидкости возникали колебан...  [учёный, разработать, метод, осязание, позволять, робот, различать, сходный, текстура, поверхность, хороший, делать, человек, работа, опубликовать, журнал, frontiers, neurorobotics, краткий, содержание, прочитать, сайт, университет, южный, калифорния, исследователь, собрать, тестовый, робот, коммерчески, доступный, компонент, заняться, создание, программный, обеспечение, который, позволить, максимально, достоверно, различать, сходный, поверхность, тактильный, сенсор, основа, который, происходить, осязание, имитировать, строение, человеческий, палец, снаружи, покрыть, эластичный, кожа, несущий, папиллярный, узор, внутри, заполнить, жидкость, внутри, находиться, датчик, тепло, сила, давление, искусственный, палец, скользить, поверхность, жидкость, возникать, колебание, который, записыват...\n",
       "38     Дель Боске продлил контракт со сборной Испании  Главный тренер сборной Испании по футболу Висенте дель Боске продлил соглашение с командой. Новый контракт будет действовать до конца чемпионата мира 2014 года, который пройдет в Бразилии, сообщает испанское издание Marca. \"Я вновь с испанской командой\", - приводит слова дель Боске Marca. Предыдущее соглашение дель Боске со сборной Испании заканчивалось после чемпионата Европы-2012. Дель Боске возглавил сборную Испании после победного чемпионата Европы-2008, заменив на этом посту Луиса Арагонеса. В 2010 году сборная Испании под руководством дель Боске впервые выиграла чемпионат мира. В финале мирового первенства, которое проходило в ЮАР, испанцы обыграли в дополнительное время сборную Нидерландов. Испанцы уверенно отобрались на чемпионат Европы-2012, который пройдет в Польше и Украине ...  [главный, тренер, сборный, испания, футбол, висенте, дель, боска, продлить, соглашение, команда, новый, контракт, действовать, конец, чемпионат, который, пройти, бразилия, сообщать, испанский, издание, marca, вновь, испанский, команда, приводить, слово, дель, боска, marca, предыдущий, соглашение, дель, боска, сборный, испания, заканчиваться, чемпионат, европа, дель, боска, возглавить, сборная, испания, победный, чемпионат, европа, заменить, пост, луис, арагонес, сборная, испания, руководство, дель, боска, впервые, выиграть, чемпионат, финал, мировой, первенство, который, проходить, испанец, обыграть, дополнительный, время, сборная, нидерланды, испанец, уверенно, отобраться, чемпионат, европа, который, пройти, польша, украина, июнь, июль, групповой, этап, турнир, испания, сыграть, итали..."
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.head(200)\n",
    "data['lemmas'] = lemmas\n",
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Итого:\n",
    "\n",
    "- посмотрели, как делать все стандартные этапы предобработки текста\n",
    "- научились работать с морфологоческими парсерами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
